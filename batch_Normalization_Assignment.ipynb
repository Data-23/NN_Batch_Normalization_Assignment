{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad550ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Q1: Concepts\n",
    "\n",
    "# #### 1. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "\n",
    "# **Answer:**\n",
    "# Batch normalization is a technique used to improve the training of deep neural networks. It involves normalizing the input layer by adjusting and scaling the activations. This process helps in reducing the internal covariate shift, which refers to the change in the distribution of network activations due to the change in network parameters during training. Batch normalization stabilizes and accelerates the training process.\n",
    "\n",
    "# #### 2. Describe the benefits of using batch normalization during training.\n",
    "\n",
    "# **Answer:**\n",
    "# - **Accelerates Training:** By normalizing the inputs, batch normalization allows for faster convergence.\n",
    "# - **Improves Stability:** Reduces the sensitivity to the initial learning rate and helps prevent exploding or vanishing gradients.\n",
    "# - **Regularization:** Acts as a form of regularization, reducing the need for dropout.\n",
    "# - **Improves Generalization:** Helps the model generalize better on unseen data.\n",
    "\n",
    "# #### 3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "# **Answer:**\n",
    "# - **Normalization Step:** For each mini-batch, batch normalization normalizes the activations to have a mean of zero and a variance of one. This is done using the batch statistics (mean and variance).\n",
    "# - **Learnable Parameters:** After normalization, two learnable parameters, gamma (scale) and beta (shift), are introduced to allow the network to learn the optimal scale and shift for the activations. The formula is:\n",
    "#   \\[\n",
    "#   \\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n",
    "#   \\]\n",
    "#   \\[\n",
    "#   y = \\gamma \\hat{x} + \\beta\n",
    "#   \\]\n",
    "#   where \\(\\mu_{\\text{batch}}\\) and \\(\\sigma_{\\text{batch}}\\) are the mean and variance of the batch, and \\(\\epsilon\\) is a small constant for numerical stability.\n",
    "\n",
    "# ### Q2: Implementation\n",
    "\n",
    "# We will use the MNIST dataset for this example. Below are the steps to implement and compare the neural network with and without batch normalization.\n",
    "\n",
    "# #### 1. Preprocess the dataset\n",
    "\n",
    "# ```python\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Load and preprocess the MNIST dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "# x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "# y_train = to_categorical(y_train, 10)\n",
    "# y_test = to_categorical(y_test, 10)\n",
    "# ```\n",
    "\n",
    "# #### 2. Implement a simple feedforward neural network without batch normalization\n",
    "\n",
    "# ```python\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# # Define the model without batch normalization\n",
    "# model_without_bn = Sequential([\n",
    "#     Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model_without_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history_without_bn = model_without_bn.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "# ```\n",
    "\n",
    "# #### 3. Implement the same neural network with batch normalization\n",
    "\n",
    "# ```python\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# # Define the model with batch normalization\n",
    "# model_with_bn = Sequential([\n",
    "#     Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model_with_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history_with_bn = model_with_bn.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "# ```\n",
    "\n",
    "# #### 4. Compare the training and validation performance\n",
    "\n",
    "# ```python\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.plot(history_without_bn.history['accuracy'], label='Without BN')\n",
    "# plt.plot(history_with_bn.history['accuracy'], label='With BN')\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot loss\n",
    "# plt.plot(history_without_bn.history['loss'], label='Without BN')\n",
    "# plt.plot(history_with_bn.history['loss'], label='With BN')\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "# ```\n",
    "\n",
    "# #### 5. Discuss the impact of batch normalization on the training process and the performance of the neural network.\n",
    "\n",
    "# **Answer:**\n",
    "# Batch normalization generally improves the training process by allowing higher learning rates and stabilizing the training process. It helps in faster convergence and often leads to better generalization on the validation data. The plots of accuracy and loss typically show smoother curves and faster convergence when batch normalization is used.\n",
    "\n",
    "# ### Q3: Experimentation and Analysis\n",
    "\n",
    "# #### 1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance\n",
    "\n",
    "# **Code for experimentation:**\n",
    "# ```python\n",
    "# batch_sizes = [32, 64, 128]\n",
    "\n",
    "# for batch_size in batch_sizes:\n",
    "#     print(f'\\nTraining with batch size: {batch_size}')\n",
    "#     model = Sequential([\n",
    "#         Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling2D(pool_size=(2, 2)),\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(10, activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     history = model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_test, y_test))\n",
    "# ```\n",
    "\n",
    "# #### 2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks.\n",
    "\n",
    "# **Answer:**\n",
    "# **Advantages:**\n",
    "# - **Stabilizes Learning:** Helps in stabilizing and speeding up the training process.\n",
    "# - **Reduces Sensitivity:** Reduces the sensitivity to initialization and learning rates.\n",
    "# - **Regularization Effect:** Acts as a form of regularization, reducing the need for techniques like dropout.\n",
    "\n",
    "# **Potential Limitations:**\n",
    "# - **Additional Computation:** Introduces additional computational overhead.\n",
    "# - **Batch Dependence:** The normalization depends on batch statistics, which might not be optimal if the batch size is too small.\n",
    "# - **Implementation Complexity:** Adds complexity to the implementation and debugging process.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
